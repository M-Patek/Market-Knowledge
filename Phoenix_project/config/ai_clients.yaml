# This file centralizes configuration for all AI-related clients and models.

# 1. AI Client Providers (e.g., Google, OpenAI)
# This section defines the API configurations for different AI model providers.
ai_providers:
  google:
    api_key_env_var: "GOOGLE_API_KEY"
    # Models available from this provider
    models:
      - name: "gemini-1.5-flash-latest"
        type: "generative"
      - name: "text-embedding-004"
        type: "embedding"

# 2. Embedding Client Configuration
# This specifies which provider and model to use for generating text embeddings.
embedding_client:
  provider: "google"
  model: "text-embedding-004"
  # Maximum number of documents to process in a single batch API call.
  batch_size: 100

# 3. Ensemble AI Client Configuration
# This configures the master client that orchestrates the reasoning ensemble.
ensemble_client:
  # Path to the detailed configuration file for the reasoning models.
  model_config_path: "ai/model_config.yaml"

# 4. Hybrid Retriever & Re-ranking Settings
retriever:
  # The constant 'k' for the Reciprocal Rank Fusion (RRF) algorithm.
  # A common value is 60. Lower values give more weight to the top-ranked items.
  rrf_k: 60

  # Decay rate for the freshness score. Higher value means faster decay.
  freshness_decay_rate: 0.05 # Roughly a 5% score reduction per day

  # Weights assigned to different document source types.
  # This allows prioritizing more authoritative sources.
  source_type_weights:
    "10-K": 1.0
    "10-Q": 0.9
    "Press Release": 0.8
    "News": 0.7
    "Analyst Report": 0.6
    "Market Data": 0.9
    "Other": 0.5

# 5. Vector Database Configuration
vector_database:
  index_name: "phoenix-project-rag"
  embedding_dimension: 768 # Default for gemini-embedding-001


# 6. Global Settings
audit_log_retention_days: 30
