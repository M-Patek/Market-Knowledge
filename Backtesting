# Phoenix Project - Final Version (Phoenix Ascendant)
# A collaborative masterpiece by Gemini & AI, guided by our Master.
# This version features a modular, robust architecture with a centralized DataManager,
# a type-safe configuration, refined error handling, and a clear separation of concerns.

import os
import datetime
import logging
import random
import requests
from typing import List, Dict, Optional
from dataclasses import dataclass, field
import concurrent.futures

import backtrader as bt
import pandas as pd
import yfinance as yf


# --- [Phase 1.1] Configuration Layer (The Command Deck) V2.0 ---
@dataclass(frozen=True)
class StrategyConfig:
    """
    Central configuration for the Phoenix Project.
    Immutable (frozen) to ensure run-to-run consistency and prevent accidental changes.
    """
    # 1. Backtest Timeframe & Universe
    start_date: str = '2022-01-01'
    end_date: str = '2024-12-31'
    asset_universe: List[str] = field(default_factory=lambda: ['QQQ', 'SPY', 'GLD', 'TLT', 'IWM'])

    # 1.5. Market Breadth Tickers
    market_breadth_tickers: List[str] = field(default_factory=lambda: ['SPY', 'QQQ', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA'])

    # 2. Strategy Parameters
    sma_period: int = 50
    opportunity_score_threshold: float = 55.0
    momentum_score_positive: float = 80.0
    momentum_score_negative: float = 30.0

    # 3. Risk Management Parameters (VIX-based)
    vix_high_threshold: float = 30.0
    vix_low_threshold: float = 20.0
    capital_modifier_high_vix: float = 0.5
    capital_modifier_normal_vix: float = 0.9
    capital_modifier_low_vix: float = 1.0

    # 4. Cerebro Engine Settings
    initial_cash: float = 1_000_000.0
    commission_rate: float = 0.001
    log_level: str = "INFO"


# --- Section 1.5: Data Management Layer (The Data Hub) ---
class DataManager:
    """
    [Phase 2.0] Handles all data fetching, caching, and pre-processing for the Phoenix Project.
    Centralizes data logic to ensure consistency and robustness.
    """
    def __init__(self, config: StrategyConfig, cache_dir: str = "data_cache"):
        self.config = config
        self.cache_dir = cache_dir
        self.logger = logging.getLogger("PhoenixProject")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.logger.info(f"DataManager initialized. Cache directory set to '{self.cache_dir}'.")

    def _fetch_with_cache(self, cache_filename: str, fetch_func, *args, **kwargs) -> Optional[pd.DataFrame]:
        """Universal caching wrapper for any data fetching function."""
        cache_path = os.path.join(self.cache_dir, cache_filename)
        if os.path.exists(cache_path):
            self.logger.info(f"Loading data from cache: {cache_path}")
            try:
                return pd.read_feather(cache_path).set_index('Date')
            except Exception as e:
                self.logger.error(f"Failed to load from cache {cache_path}: {e}. Refetching.")

        self.logger.info(f"Cache not found for '{cache_filename}'. Fetching fresh data...")
        data = fetch_func(*args, **kwargs)
        
        if data is None or data.empty:
            self.logger.warning(f"Fetching function for '{cache_filename}' returned no data.")
            return None

        data.reset_index().to_feather(cache_path)
        self.logger.info(f"Saved fresh data to cache: {cache_path}")
        return data

    def get_yfinance_data(self, tickers: List[str] or str, start: str, end: str) -> Optional[pd.DataFrame]:
        """A robust wrapper for yf.download with refined error handling."""
        try:
            df = yf.download(tickers, start=start, end=end, progress=False)
            if df.empty:
                 self.logger.warning(f"yfinance returned an empty DataFrame for ticker(s): {tickers}.")
            return df
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"Network ConnectionError for {tickers}: {e}. Check internet connection.")
            return None
        except Exception as e:
            self.logger.critical(f"An unexpected error occurred during yfinance download for {tickers}: {e}", exc_info=True)
            return None

    def get_vix_data(self) -> Optional[pd.Series]:
        df = self._fetch_with_cache("vix_data.feather", self.get_yfinance_data, tickers='^VIX', start=self.config.start_date, end=self.config.end_date)
        return df['Close'] if df is not None and 'Close' in df else None

    def get_treasury_yield_data(self) -> Optional[pd.Series]:
        df = self._fetch_with_cache("treasury_yield_data.feather", self.get_yfinance_data, tickers='^TNX', start=self.config.start_date, end=self.config.end_date)
        return df['Close'] if df is not None and 'Close' in df else None
        
    def get_asset_universe_data(self) -> Optional[pd.DataFrame]:
        """Fetches data for the main asset universe defined in the config."""
        return self._fetch_with_cache("asset_universe_data.feather", self.get_yfinance_data, tickers=self.config.asset_universe, start=self.config.start_date, end=self.config.end_date)

    def get_market_breadth_data(self) -> Optional[pd.Series]:
        """Calculates and caches the market breadth indicator."""
        cache_filename = f"market_breadth_sma{self.config.sma_period}.feather"
        cache_path = os.path.join(self.cache_dir, cache_filename)
        if os.path.exists(cache_path):
            self.logger.info(f"Loading final market breadth from cache: {cache_path}")
            return pd.read_feather(cache_path).set_index('Date').squeeze()

        self.logger.info("Calculating market breadth...")
        # Fetch prices for tickers defined in the configuration for market breadth calculation.
        all_prices_df_container = self._fetch_with_cache("sp500_prices.feather", self.get_yfinance_data, tickers=self.config.market_breadth_tickers, start=self.config.start_date, end=self.config.end_date)
        
        if all_prices_df_container is None or 'Close' not in all_prices_df_container:
            return None
        all_prices_df = all_prices_df_container['Close']

        smas = all_prices_df.rolling(window=self.config.sma_period).mean()
        is_above_sma = all_prices_df > smas
        breadth_series = (is_above_sma.sum(axis=1) / all_prices_df.notna().sum(axis=1)).fillna(0)
        
        breadth_series.to_frame(name='breadth').reset_index().to_feather(cache_path)
        self.logger.info(f"Saved final market breadth to cache: {cache_path}")
        return breadth_series

    def get_aligned_data(self) -> Optional[Dict[str, pd.DataFrame | pd.Series]]:
        """
        [Phase 2.1] The core data pre-processing hub.
        Gathers, aligns, and sanitizes all data before backtesting.
        """
        self.logger.info("--- Starting data alignment and sanitization ---")
        
        # Step 1: Fetch asset universe data first, as it defines the master index. This must be sequential.
        asset_df = self.get_asset_universe_data()
        if asset_df is None or asset_df.empty:
            self.logger.critical("Cannot create master index: Asset universe data is missing.")
            return None
        master_index = asset_df.index
        self.logger.info(f"Master trading index created with {len(master_index)} days.")

        # Step 2: Fetch all other auxiliary data streams in parallel.
        data_streams = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_name = {
                executor.submit(self.get_vix_data): "vix",
                executor.submit(self.get_treasury_yield_data): "treasury_yield",
                executor.submit(self.get_market_breadth_data): "market_breadth"
            }
            for future in concurrent.futures.as_completed(future_to_name):
                name = future_to_name[future]
                data_streams[name] = future.result()
    
        # Step 3: Align and sanitize all data streams as before.
        sanitized_streams = {}    
        for name, series in data_streams.items():
            aligned_series = series.reindex(master_index) if series is not None else pd.Series(index=master_index)
            sanitized_streams[name] = aligned_series.ffill().bfill()
            if sanitized_streams[name].isnull().any():
                self.logger.warning(f"{name} data still contains NaNs after sanitization.")
        
        self.logger.info("--- Data alignment and sanitization complete ---")
        return {"asset_universe_df": asset_df, **sanitized_streams}

# --- Section 2: Gemini Cognitive Engine (The Marshal's Brain) ---
class CognitiveEngine:
    """[Phase 3.3] The central coordinating brain of the strategy.
    Encapsulates all decision-making logic by coordinating specialized managers.
    """
    def __init__(self, config: StrategyConfig):
        self.config = config
        self.logger = logging.getLogger("PhoenixProject")
        self.risk_manager = RiskManager(config)
        self.portfolio_constructor = PortfolioConstructor(config)

    def determine_allocations(self, candidate_analysis: List[Dict], current_vix: float) -> List[Dict]:
        """The primary entry point for the engine's decision-making process."""
        self.logger.info("--- [Cognitive Engine Call: Marshal Coordination] ---")
        capital_modifier = self.risk_manager.get_capital_modifier(current_vix)
        return self.portfolio_constructor.construct_portfolio(candidate_analysis, capital_modifier)

    def analyze_asset_momentum(self, current_price: float, current_sma: float) -> float:
        """A proxy method to access the momentum analysis function."""
        return self.portfolio_constructor.analyze_asset_momentum(current_price, current_sma, self.config)

class RiskManager:
    """[Phase 3.1] Encapsulates all market risk assessment logic."""
    def __init__(self, config: StrategyConfig):
        self.config = config
        self.logger = logging.getLogger("PhoenixProject")

    def get_capital_modifier(self, current_vix: float) -> float:
        """Returns a capital modifier based on the current VIX level."""
        self.logger.info(f"RiskManager is assessing VIX: {current_vix:.2f}")
        if current_vix > self.config.vix_high_threshold:
            self.logger.info("RiskManager Read: High fear. Defensive stance.")
            return self.config.capital_modifier_high_vix
        elif current_vix < self.config.vix_low_threshold:
            self.logger.info("RiskManager Read: Low fear. Aggressive stance.")
            return self.config.capital_modifier_low_vix
        else:
            self.logger.info("RiskManager Read: Normal fear. Standard operations.")
            return self.config.capital_modifier_normal_vix

class PortfolioConstructor:
    """[Phase 3.2] Encapsulates the logic for constructing the target portfolio."""
    def __init__(self, config: StrategyConfig):
        self.config = config
        self.logger = logging.getLogger("PhoenixProject")

    @staticmethod
    def analyze_asset_momentum(current_price: float, current_sma: float, config: StrategyConfig) -> float:
        """ANALYST V1.4: Analyzes a single asset's momentum. Now depends on the config object."""
        return config.momentum_score_positive if current_price > current_sma else config.momentum_score_negative

    def construct_portfolio(self, candidate_analysis: List[Dict], capital_modifier: float) -> List[Dict]:
        """Filters candidates and calculates final capital allocation."""
        self.logger.info("PortfolioConstructor is analyzing candidates...")
        worthy_targets = [res for res in candidate_analysis if res["opportunity_score"] > self.config.opportunity_score_threshold]
        if not worthy_targets:
            self.logger.info("PortfolioConstructor: No high-quality opportunities found. Standing down.")
            return []
        total_score = sum(t['opportunity_score'] for t in worthy_targets)
        battle_plan = []
        for target in worthy_targets:
            base_allocation = target['opportunity_score'] / total_score
            final_allocation = base_allocation * capital_modifier
            battle_plan.append({"ticker": target['ticker'], "capital_allocation_pct": final_allocation})
        
        self.logger.info("--- [PortfolioConstructor's Final Battle Plan] ---")
        total_planned_allocation = sum(d['capital_allocation_pct'] for d in battle_plan)
        self.logger.info(f"Total planned capital deployment today: {total_planned_allocation:.2%}")
        for deployment in battle_plan:
            self.logger.info(f"- Asset: {deployment['ticker']}, Deploy Capital: {deployment['capital_allocation_pct']:.2%}")
        return battle_plan

# --- Section 3: Strategy Execution Layer (The Roman Legion) ---
class RomanLegionStrategy(bt.Strategy):
    params = (('config', None), ('vix_data', None), ('treasury_yield_data', None), ('market_breadth_data', None))

    def __init__(self):
        self.logger = logging.getLogger("PhoenixProject")
        if self.p.config is None: raise ValueError("StrategyConfig object not provided!")
        self.config = self.p.config
        self.cognitive_engine = CognitiveEngine(self.config)
        self.data_map = {d._name: d for d in self.datas}
        self.sma_indicators = {d._name: bt.indicators.SimpleMovingAverage(d.close, period=self.config.sma_period) for d in self.datas}

    def start(self):
        self.logger.info(f"{self.datas[0].datetime.date(0).isoformat()}: [Legion Commander]: Awaiting daily orders...")

    def next(self):
        if len(self.datas[0]) < self.config.sma_period: return
        current_date = self.datas[0].datetime.date(0)
        self.logger.info(f"--- {current_date.isoformat()}: Daily Rebalancing Briefing ---")

        current_vix = self.p.vix_data.get(current_date)
        current_yield = self.p.treasury_yield_data.get(current_date)
        current_breadth = self.p.market_breadth_data.get(current_date)

        if current_vix is None:
            self.logger.warning(f"Critical data VIX missing for {current_date}, halting for the day.")
            return

        self.logger.info(f"VIX Index: {current_vix:.2f}, 10Y Yield: {current_yield:.2f}%, Market Breadth: {current_breadth:.2%}")

        candidate_analysis = [
            {"ticker": ticker, "opportunity_score": self.cognitive_engine.analyze_asset_momentum(d.close[0], self.sma_indicators[ticker][0])}
            for ticker, d in self.data_map.items()
        ]
        battle_plan = self.cognitive_engine.determine_allocations(candidate_analysis, current_vix)

        self.logger.info("--- Starting Unified Rebalancing Protocol ---")
        total_value = self.broker.getvalue()
        target_portfolio = {deployment['ticker']: total_value * deployment['capital_allocation_pct'] for deployment in battle_plan}
        
        current_positions = {pos.data._name for pos in self.positions if self.getposition(pos).size != 0}
        target_tickers = set(target_portfolio.keys())
        all_tickers_in_play = current_positions.union(target_tickers)

        for ticker in all_tickers_in_play:
            target_value = target_portfolio.get(ticker, 0.0)
            self.logger.info(f"Rebalance SYNC: Aligning {ticker} to target value ${target_value:,.2f}.")
            self.order_target_value(target=target_value, data=self.getdatabyname(ticker))
        self.logger.info("--- Rebalancing Protocol Concluded ---")

    def notify_order(self, order):
        if order.status in [order.Completed]:
            dt = self.datas[0].datetime.date(0).isoformat()
            if order.isbuy(): self.logger.info(f"{dt}: BUY EXECUTED, {order.data._name}, Size: {order.executed.size}, Price: {order.executed.price:.2f}")
            elif order.issell(): self.logger.info(f"{dt}: SELL EXECUTED, {order.data._name}, Size: {order.executed.size}, Price: {order.executed.price:.2f}")
        elif order.status in [order.Canceled, order.Margin, order.Rejected]:
            self.logger.warning(f"{self.datas[0].datetime.date(0).isoformat()}: Order for {order.data._name} failed: {order.getstatusname()}")

# --- Section 4: Main Execution Engine (The High Command) ---
if __name__ == '__main__':
    config = StrategyConfig()
    
    log_level = getattr(logging, config.log_level.upper(), logging.INFO)
    logger = logging.getLogger("PhoenixProject")
    logger.setLevel(log_level)
    if not logger.handlers:
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = f"phoenix_project_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)
        
        file_handler = logging.FileHandler(os.path.join(log_dir, log_filename))
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    logger.info("Phoenix Project Final Version - Logging System Initialized.")

    data_manager = DataManager(config)
    all_aligned_data = data_manager.get_aligned_data()

    if not all_aligned_data:
        logger.critical("Failed to get aligned data. Aborting operation.")
    else:
        cerebro = bt.Cerebro()
        
        try:
            all_data_df = all_aligned_data["asset_universe_df"]
            if all_data_df is None or all_data_df.empty: raise ValueError("Asset universe data is empty.")
            unique_tickers = all_data_df.columns.get_level_values(1).unique()
            for ticker in unique_tickers:
                ticker_df = all_data_df.xs(ticker, level=1, axis=1).copy()
                ticker_df.columns = [col.lower() for col in ticker_df.columns]
                ticker_df.dropna(inplace=True)
                if not ticker_df.empty:
                    logger.info(f"Adding data feed for {ticker} with {len(ticker_df)} bars.")
                    cerebro.adddata(bt.feeds.PandasData(dataname=ticker_df, name=ticker))
        except Exception as e:
            logger.critical(f"A critical error occurred during data loading: {e}. Aborting.")
            exit()

        if not cerebro.datas:
            logger.critical("Failed to load data for any asset. Aborting operation.")
        else:
            cerebro.addstrategy(
                RomanLegionStrategy, 
                config=config,
                vix_data=all_aligned_data["vix"],
                treasury_yield_data=all_aligned_data["treasury_yield"],
                market_breadth_data=all_aligned_data["market_breadth"]
            )
            cerebro.broker.setcash(config.initial_cash)
            cerebro.broker.setcommission(commission=config.commission_rate)
            
            cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe_ratio')
            cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
            cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')
            cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')

            logger.info('--- Launching "Phoenix Project" (Ascendant Version) ---')
            results = cerebro.run()
            
            logger.info("--- Operation Concluded ---")
            strat = results[0]
            trade_analysis = strat.analyzers.trade_analyzer.get_analysis()
            total_trades = trade_analysis.total.get('total', 0)
            win_rate = (trade_analysis.won.get('total', 0) / total_trades) if total_trades > 0 else 0

            report = f"""
\n================== PHOENIX PROJECT: AFTER-ACTION REPORT ==================
Final Empire Assets: ${cerebro.broker.getvalue():,.2f}
--------------------------------------------------------------------------
Performance Metrics:
- Total Return: {strat.analyzers.returns.get_analysis().get('rtot', 0.0) * 100:.2f}%
- Sharpe Ratio: {strat.analyzers.sharpe_ratio.get_analysis().get('sharperatio', 'N/A')}
- Max Drawdown: {strat.analyzers.drawdown.get_analysis().max.get('drawdown', 0.0):.2f}%
--------------------------------------------------------------------------
Trade Statistics:
- Total Trades: {total_trades}
- Winning Trades: {trade_analysis.won.get('total', 0)}
- Losing Trades: {trade_analysis.lost.get('total', 0)}
- Win Rate: {win_rate:.2%}
==========================================================================
"""
            logger.info(report)
            cerebro.plot(style='candlestick', barup='green', bardown='red')
